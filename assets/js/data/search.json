[ { "title": "About", "url": "/posts/about/", "categories": "", "tags": "", "date": "2023-03-16 00:00:00 +0000", "snippet": "AboutThank you for using ZMediumToJekyll, this tool is powered by ZMediumToMarkdown, that is also create by ZhgChgLi.This tool can help you move your Medium posts to a Jekyll blog and keep them in ...", "content": "AboutThank you for using ZMediumToJekyll, this tool is powered by ZMediumToMarkdown, that is also create by ZhgChgLi.This tool can help you move your Medium posts to a Jekyll blog and keep them in sync in the future.It will automatically download your posts from Medium, convert them to Markdown, and upload them to your repository, check out my blog for online demo zhgchg.li.One-time setting, Lifetime enjoying‚ù§Ô∏èPowered by ZMediumToMarkdown, build on Jekyll with Chirpy theme.If you find this tool helpful, please consider to buy me a coffee.‚ù§Ô∏èSetup You can follow along with each step of this process by watching the following video tutorial Click the green button Use this template located above and select Create a new repository. Repo Owner could be an organization or username Enter the Repository Name, which usually uses your GitHub Username/Organization Name and ends with .github.io, for example, my organization name is zhgchgli than it‚Äôll be zhgchgli.github.io. Select the public repository option, and then click on Create repository from template. Grant access to GitHub Actions by going to the Settings tab in your GitHub repository, selecting Actions -&gt; General, and finding the Workflow permissions section, then, select Read and write permissions, and click on Save to save the changes.*If you choose a different Repository Name, the GitHub page will be https://username.github.io/Repository Name instead of https://username.github.io/, and you will need to fill in the baseurl field in _config.yml with your Repository Name.*If you are using an organization and cannot enable Read and Write permissions in the repository settings, please refer to the organization settings page and enable it there.First-time run Please refer to the configuration information in the section below and make sure to specify your Medium username in the _zmediumtomarkdown.yml file. ‚åõÔ∏è Please wait for the Automatic Build and pages-build-deployment gitHub actions to finish before making any further changes. Then, you can manually run the ZMediumToMarkdown GitHub action by going to the Actions tab in your GitHub repository, selecting the ZMediumToMarkdown action, clicking on the Run workflow button, and selecting the main branch. ‚åõÔ∏è Please wait for the action to download and convert all Medium posts from the specified username, and commit the posts to your repository. ‚åõÔ∏è Please wait for the Automatic Build and pages-build-deployment actions will also need to finish before making any further changes, and that they will start automatically once the ZMediumToMarkdown action has completed. Go to the Settings section of your GitHub repository and select Pages, In the Branch field, select gh-pages, and leave /(root) selected as the default. Click Save, you can also find the URL for your GitHub page at the top of the page. ‚åõÔ∏è Please wait the Pages build and deployment action to finish. üéâ After all actions are completed, you can visit your xxx.github.io page to verify that the results are correct. Congratulations! üéâ*To avoid expected Git conflicts or unexpected errors, please follow the steps carefully and in order, and be patient while waiting for each action to complete.*Note that the first time running may take longer.*If you open the URL and notice that something is wrong, such as the web style being missing, please ensure that your configuration in the _config.yml file is correct.*Please refer to the ‚ÄòThings to Know‚Äô and ‚ÄòTroubleshooting‚Äô sections below for more information.ConfigurationSite Setting_zmediumtomarkdown.ymlmedium_username: # enter your username on Medium.comPlease specify your Medium username for automatic download and syncing of your posts._config.yml &amp; jekyll settingFor more information, please refer to jekyll-theme-chirpy or jekyllrb.Github ActionZMediumToMarkdownYou can configure the time interval for syncing in ./.github/workflows/ZMediumToMarkdown.yml.The default time interval for syncing is once per day.You can also manually run the ZMediumToMarkdown action by going to the Actions tab in your GitHub repository, selecting the ZMediumToMarkdown action, clicking on the Run workflow button, and selecting the main branch.DisclaimerAll content downloaded using ZMediumToMarkdown, including but not limited to articles, images, and videos, are subject to copyright laws and belong to their respective owners. ZMediumToMarkdown does not claim ownership of any content downloaded using this tool.Downloading and using copyrighted content without the owner‚Äôs permission may be illegal and may result in legal action. ZMediumToMarkdown does not condone or support copyright infringement and will not be held responsible for any misuse of this tool.Users of ZMediumToMarkdown are solely responsible for ensuring that they have the necessary permissions and rights to download and use any content obtained using this tool. ZMediumToMarkdown is not responsible for any legal issues that may arise from the misuse of this tool.By using ZMediumToMarkdown, users acknowledge and agree to comply with all applicable copyright laws and regulations.TroubleshootingMy GitHub page keeps presenting a 404 error or doesn‚Äôt update with the latest posts. Please make sure you have followed the setup steps above in order. Wait for all GitHub actions to finish, including the Pages build and deployment and Automatic Build actions, you can check the progress on the Actions tab. Make sure you have the correct settings selected in Settings -&gt; Pages.Things to know The ZMediumToMarkdown GitHub Action for syncing Medium posts will automatically run every day by default, and you can also manually trigger it on the GitHub Actions page or adjust the sync frequency as needed. Every commit and post change will trigger the Automatic Build &amp; Pages build and deployment action. Please wait for this action to finish before checking the final result. You can create your own Markdown posts in the _posts directory by naming the file as YYYY-MM-DD-POSTNAME and recommend using lowercase file names. You can include images and other resources in the /assets directory. Things to know The ZMediumToMarkdown GitHub Action for syncing Medium posts will automatically run every day by default, and you can also manually trigger it on the GitHub Actions page or adjust the sync frequency as needed. Every commit and post change will trigger the Automatic Build &amp; Pages build and deployment action. Please wait for this action to finish before checking the final result. You can create your own Markdown posts in the _posts directory by naming the file as YYYY-MM-DD-POSTNAME and recommend using lowercase file names. You can include images and other resources in the /assets directory. Also, if you would like to remove the ZMediumToMarkdown watermark located at the bottom of the post, you may do so. I don‚Äôt mind. You can edit the Ruby file at tools/optimize_markdown.rb and uncomment lines 10-12. This will automatically remove the ZMediumToMarkdown watermark at the end of all posts during Jekyll build time. Since ZMediumToMarkdown is not an official tool and Medium does not provide a public API for it, I cannot guarantee that the parser target will not change in the future. However, I have tried to test it for as many cases as possible. If you encounter any rendering errors or Jekyll build errors, please feel free to create an issue and I will fix them as soon as possible." }, { "title": "A few notes on Combinatorics", "url": "/posts/a31c3d69a21d/", "categories": "", "tags": "combinatorics, probability, counting, mathematics", "date": "2020-06-18 17:34:51 +0000", "snippet": "A few notes on CombinatoricsCombinatorics is about counting the number of ways things can be done. It is very useful when dealing with discrete probability and equally likely outcomes.The number of...", "content": "A few notes on CombinatoricsCombinatorics is about counting the number of ways things can be done. It is very useful when dealing with discrete probability and equally likely outcomes.The number of ways for choosing r elements from a set of n elements is: Top-right is permutations (ordered) . Bottom-right is combinations (unordered) .Post converted from Medium by ZMediumToMarkdown." }, { "title": "A few notes of mathematics", "url": "/posts/305951b2b4ee/", "categories": "", "tags": "mathematics, problem-solving, logic, math, thinking", "date": "2019-12-26 15:28:48 +0000", "snippet": "A few notes of mathematicsMathematical StatementsDefinitions and Propositions/Theorems/‚Ä¶ normally take the form: Object a has property P .3 is prime. Every object of type T has property P .Every ...", "content": "A few notes of mathematicsMathematical StatementsDefinitions and Propositions/Theorems/‚Ä¶ normally take the form: Object a has property P .3 is prime. Every object of type T has property P .Every integer ‚â•1 is prime or can be represented as the product of primes, uniquely. There exist objects of type T having property P .There exist infinite sets with different sizes. If statement A , then statement B .If a function f is both injective and surjective, then f is bijection.List some problem solving strategies What are you trying to answer?Do you understand all notation/terminology?Can you draw a table/graph/picture to describe what‚Äôs going on? Can you identify exactly why you are stuck?Simplify the problem/consider simple cases (when x = 0, 1, 2,‚Ä¶; when x is real, integer,‚Ä¶; limiting cases) . Make conjectures and assumptions ‚Äî do they initially break down?Which theorems can be used based on the assumptions? Patterns? Similar examples?What strategies are used for similar problems like this? Is it better to work backwards? Create a concept map (quantity not quality) .Which ideas are worth following? Take a break/walk (use the deep mind to solve unconsciously) . Review:Check all calculations and logic hold.Dimensional analysis: Is the answer dimensionally correct? Does the size/sign seem reasonable?Can you check using a different method?Post converted from Medium by ZMediumToMarkdown." }, { "title": "Types of software optimisations", "url": "/posts/cce8a3ba4742/", "categories": "", "tags": "programming, computer-science, optimization, software-development, performance", "date": "2019-09-14 11:56:41 +0000", "snippet": "Types of software optimisationsCPU Bound Better data structures (searching) and algorithms (sorting) Minimisation of in-loop tasks, parallelisation Better compilation/low-level optimisations- Mo...", "content": "Types of software optimisationsCPU Bound Better data structures (searching) and algorithms (sorting) Minimisation of in-loop tasks, parallelisation Better compilation/low-level optimisations- Move closer to hardware (NumPy/C, Numba, Cython)- Faster hardware (GPU, TPU)I/O Bound I/O (disk, network, etc. ) access optimisation- File formats ( .parquet for tabular, .zarr for arrays) Compression (when over network) ParallelisationMemory Bound Memory access optimisations, caches- functools.lru_cache (Least Recently Used; more generally: https://en.wikipedia.org/wiki/Cache_replacement_policies )- functools.cached_property Compression- dtypes, removing irrelevant columns in tabular dataProgrammer Bound Code readability, styles, etc. Use of libraries Parallelisation (teamwork)Profiling: For an accurate measurement, the benchmark should be designed to have a long enough execution time (in the order of seconds) so that the setup and tear-down of the process is small compared to the execution time of the application.Jeff Dean Tips: https://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/stanford-295-talk.pdfPost converted from Medium by ZMediumToMarkdown." }, { "title": "A few notes from Andrew Ng‚Äôs ‚ÄúAI For Everyone‚Äù", "url": "/posts/8909af941636/", "categories": "", "tags": "machine-learning, data-science, artificial-intelligence, ai, strategy", "date": "2019-07-21 11:24:09 +0000", "snippet": "A few notes from Andrew Ng‚Äôs ‚ÄúAI For Everyone‚Äùhttps://www.coursera.org/learn/ai-for-everyoneAI Strategy Strategic data acquisition Unified data warehouse New roles (e.g. Machine Learning Enginee...", "content": "A few notes from Andrew Ng‚Äôs ‚ÄúAI For Everyone‚Äùhttps://www.coursera.org/learn/ai-for-everyoneAI Strategy Strategic data acquisition Unified data warehouse New roles (e.g. Machine Learning Engineer)ML tends to work well when: Learning a ‚Äúsimple‚Äù concept Lot of data available Think about automating tasks rather than automating jobs. (What are the main drivers of business value? What are the main pain points in your business?)Key steps of a Machine Learning project: Collect data Train model (iterate many times until good enough) Deploy model (get data back; maintain/update model)Key steps of a Data Science project: Collect data Analyse data (iterate many times to get good insights) Suggest hypotheses/actions (deploy changes; re-analyze new data periodically)Post converted from Medium by ZMediumToMarkdown." }, { "title": "What is the most important abstract concept in the world?", "url": "/posts/780fe901d5d5/", "categories": "", "tags": "abstract, abstraction, computing, philosophy, data", "date": "2018-08-15 15:56:08 +0000", "snippet": "What is the most important abstract concept in the world?TL;DR ‚Äî I believe Data (computing) .Wikipedia says ‚Äú Abstraction in its main sense is a conceptual process by which general rules and concep...", "content": "What is the most important abstract concept in the world?TL;DR ‚Äî I believe Data (computing) .Wikipedia says ‚Äú Abstraction in its main sense is a conceptual process by which general rules and concepts are derived from the usage and classification of specific examples. ‚Äù So one could argue that the most abstract thing is where the process of abstraction converges to a single point for any (specific) example; this unique point I believe is data.In particular, I feel that everything could be encoded as data e.g. (from the Wikipedia example https://en.wikipedia.org/wiki/Ab. . . ) (0) Data (to observe this step, note that the publication was most likely written on a computer, or could easily be inputted into a computer ‚Äî which represents everything as binary data) (1) a publication (2) a newspaper (3) The San Francisco Chronicle (4) the May 18 edition of the The San Francisco Chronicle (5) my copy of the May 18 edition of the The San Francisco Chronicle (6) my copy of the May 18 edition of the The San Francisco Chronicle as it was when I first picked it up (as contrasted with my copy as it was a few days later: in my fireplace, burning)Other things that we have converted to binary data for manipulating via a computer with include: sound waves, the rules of the universe for simulating the Big Bang, and in some sense the building blocks of life is DNA=data, also.Thus, can data explain everything? I believe is can (perhaps we are just living inside The Matrix , and ‚ÄúOur external physical reality is a mathematical universe (https://en.wikipedia.org/wiki/Mathematical_universe_hypothesis)‚Äù) . But I think the really important question is how do we efficiently encode things as data (e.g. a human brain) for easy manipulation on a computer?Post converted from Medium by ZMediumToMarkdown." }, { "title": "What are the odds for the horse race card game?", "url": "/posts/939a67602d2e/", "categories": "", "tags": "mathematics, probability, python, card-game, betting", "date": "2018-08-15 15:44:34 +0000", "snippet": "What are the odds for the horse race card game?My father challenged me to the following problem ‚Äî what are the odds for each ‚Äúhorse‚Äù in the following game scenario:1. The 4 aces (‚Äúhorses‚Äù) are laid...", "content": "What are the odds for the horse race card game?My father challenged me to the following problem ‚Äî what are the odds for each ‚Äúhorse‚Äù in the following game scenario:1. The 4 aces (‚Äúhorses‚Äù) are laid face-up at 1 end of the table.2. 6 cards face-up are laid (‚Äúrace track‚Äù) in a straight line perpendicular to the aces.3. The race proceeds by flipping the remaining deck, &amp; the ace matching that suit advances 1 step until a winner reaches the finish line (card 6) .[ TL;DL the probability of each horse winning is essentially governed by a multinomial distribution , however we must take into consideration that the probabilities of flipped cards change (since the probability of future flips depends on what has already been flipped) . ][Unfortunately Medium does not support LaTex/mathemetics yet, so some parts below may be difficult to read. ]Here is a solution that describes: Firstly how to calculate the number of sequences that are possible from each (winning) combination of cards. Using these counts for the number of sequences, we compute the probability of observing such a combination of cards based on the remaining cards in the (unflipped) deck. Finally we sum these individual probabilities of each combination to establish the overall probability each horse winning, before converting them into betting odds.Initial Remarks: The more cards of a particular suit showing on the track, the less likely that suit will be flipped within the remaining deck and so the less likely that suit will win (i.e. higher odds. ) There are 52- (4\\mbox{ aces} +6\\mbox{ racetrack cards} )=42 cards remaining in the pack.Step1: Calculating the number of sequences that are possible from each (winning) combination of cards.First consider the case for the heart horse to win. Let the sequence of flipped cards be denoted by H:=\\mbox{‚ÄúHeart card‚Äù}, C:=\\mbox{‚ÄúClub card‚Äù}, D:=\\mbox{‚ÄúDiamond card‚Äù} and S:=\\mbox{‚ÄúSpade card‚Äù} .For hearts to win, we require a combination of 6H‚Äôs with at most 5C‚Äôs, 5D‚Äôs and 5S‚Äôs to occur. Each such combination can arise from various sequences of flipped cards.We can count how many sequences arise from a particular combination of cards in the following way; note the last card of the sequence MUST always be a H for the heart horse to win (as the game finishes when we have a winner), thus in our dicussion of sequences below we omit this 6th H and only refer to the 5 other flipped H‚Äôs (e.g. the first sequence, 5H‚Äôs, refer to winning sequence {HHHHH} + {H} ) 5H‚Äôs (and no C‚Äôs, D‚Äôs or S‚Äôs flipped), i.e. {HHHHH} . Note there is only 1 way for this combination of cards (in particular the order of the H‚Äôs do not matter as each H has the same effect of advancing the heart horse by one step) . Although we can simply count the number of possible sequences for this particular combination of cards, as the combinations of cards increases and becomes more complicated, we can use the following equation to calculate this: {n\\choose r}:=\\frac{n! } {r! (n-r) ! }= ‚ÄùThe number of ways for positioning r cards within a total of n cards (where order doesn‚Äôt matter)‚Äù Indeed, for the combination of 5Hs, the number of ways for positioning the r=5 H cards within the total of n=5 cards (which in this case are just the same 5H‚Äôs) is {5\\choose 5}=\\frac{5! } {5! (5‚Äì5) ! }= 1 way . 1S, 5H‚Äôs , i.e. {SHHHHH, HSHHHH, HHSHHH, HHHSHH, HHHHSH, HHHHHS}= 6 ways . Again, we can confirm this using the equation above; note here we must also consider the positioning of the 1S: The number of ways for positioning the r=5 H cards within the total of n=6 cards (i.e. the 5H‚Äôs and the 1S) is {6\\choose 5}=\\frac{6! } {5! (6‚Äì5) ! }= 6 ways . The number of ways for positioning the r=1 S card within the remaining n=1 card is {1\\choose 1}= 1 way . Note n=1 now as we have fixed 5 positions for the 5H‚Äôs, and so there is only one remaining spot available for the 1S which it is forced to be positioned at.(E.g. if we fix the positions for the 5H‚Äôs to be the first 5 places , { HHHHH _ }, then the 1S is forced to the 6th position , {HHHHH S } ) Thus, overall there are 6\\times 1= 6 ways for positioning the 5H‚Äôs and 1S. 2S‚Äôs, 5H‚Äôs , i.e. {SSHHHHH, SHSHHHH, SHHSHHH, ‚Ä¶, HHHHHSS} = {7\\choose 5} \\times{2\\choose 2}=21\\times 1= 21 ways following the same logic as above. ‚Ä¶ 5S‚Äôs, 5H‚Äôs , i.e. {SSSSSHHHHH, SSSSHSHHHH, SSSHHSSHHH, ‚Ä¶, HHHHHSSSSS}={10\\choose 5} \\times{5\\choose 5}=252\\times 1= 252ways . 1C, 1S, 5H‚Äôs , i.e. {CSHHHHH, CHSHHHH, CHHSHHH, CHHHSHH, CHHHHSH, CHHHHHS, SCHHHHH, ‚Ä¶, HHHHHSC}= {7\\choose 5} \\times{2\\choose 1} \\times{1\\choose 1}=21\\times 2\\times 1= 42 ways . ‚Ä¶ 5S‚Äôs, 5D‚Äôs, 5C‚Äôs, 5H‚Äôs , which has {20\\choose 5} \\times{15\\choose 5} \\times{10\\choose 5} \\times{5\\choose 5} possible sequences.Recall, we stop at this combination of cards since if we have more than 6 cards for any suit other than H, the race would have already ended with a (non-heart) winner.Following this logic, we see that the number of sequences given any combination of cards can be computed as follows (let n_H, n_C, n_D, n_S denote the number of flipped H,C,D,S respectively): Thre are {n_H+n_C+n_D+n_S\\choose n_H} possible positions for the n_H Heart cards. Then once the n_H H‚Äôs have been positioned, there remains {n_C+n_D+n_S\\choose n_C} possible positions for the n_C C‚Äôs. Likewise {n_D+n_S\\choose n_D} and {n_S\\choose n_S} possible positions for the remaining n_D D‚Äôs and n_S S‚Äôs respectively. Consequently, the number of sequences for a combination of n_H H‚Äôs, n_C C‚Äôs, n_D D‚Äôs, n_S S‚Äôs is: {n_H+n_C+n_D+n_S\\choose n_H} \\times{n_C+n_D+n_S\\choose n_C} \\times{n_D+n_S\\choose n_D} \\times{n_S\\choose n_S}[We can simplify this further by expanding the equations and cancelling common terms in the numerator and denominator (an exercise left for the reader), to obtain \\frac{ (n_H+n_C+n_D+n_S) ! } {n_H!n_C!n_D!n_S! } ]Step2: Using these counts for the number of sequences, we compute the probability of observing such a combination of cards.Let: r_H denote the number of remaining H cards. n_H denote the number of flipped H (as before)and recall initially there are 42 cards remaining in the pack, then for e.g. the combination of 6H‚Äôs (note we include the 6th H in the probability calculations as they do depend on how likely this last H is flipped):\\begin{eqnarray* } \\mbox{prob( \\ {HHHHHH\\ } ) } &amp; = &amp; \\mbox{prob(‚ÄúObserving a combination of 6H‚Äôs (any order)‚Äù) } \\times\\mbox{ (the number of possible sequences 6H‚Äôs) } \\ \\ &amp; = &amp; \\frac{r_ {H} } {42} \\times\\frac{r_ {H} -1} {42‚Äì1} \\times\\frac{r_ {H} -1‚Äì1} {42‚Äì1‚Äì1} \\times\\frac{r_ {H} -1‚Äì1‚Äì1} {42‚Äì1‚Äì1‚Äì1} \\times\\frac{r_ {H} -1‚Äì1‚Äì1‚Äì1} {42‚Äì1‚Äì1‚Äì1‚Äì1} \\times\\frac{r_ {H} -1‚Äì1‚Äì1‚Äì1‚Äì1} {42‚Äì1‚Äì1‚Äì1‚Äì1‚Äì1} \\times1\\end{eqnarray* }Recall ‚Äú(the number of possible sequences 6 H‚Äôs‚Äù) was found in step1, and was equal to 1 in the case of 6H‚Äôs (i.e. 5H‚Äôs and the fixed last one) . Note the ‚Äúminus 1‚Äôs‚Äù represent each ‚Äúflip‚Äù as there is no replacement on flipping each card.We can tidy this up by using the notation P(r,n):=\\frac{r! } { (r-n) ! } (also known as the permutation ) so that:\\mbox{prob} ( {HHHHHH} )=\\frac{r_H} {42} \\times\\frac{ \\frac{r_H! } { (r_H-n_H) ! } } { \\frac{42! } { (42-n_H) ! } }=\\frac{P(r_H,n_H) } {P(42,n_H) }Now letting: r_C, r_D, r_S denote the number of remaining C, D and S cards, respectively. n_C, n_D, n_S, denote the number of flipped C, D and S, respectively. n_ {Flip}:=n_H+n_C+n_D+n_S=\\mbox{‚ÄúTotal number of cards flipped‚Äù} ):and following logic as above for computing the probability of the combination {HHHHHH}, we see that the probability for observing any combination of cards is:\\frac{P(r_H,n_H) \\times P(r_C,n_C) \\times P(r_D,n_D) \\times P(r_S,n_S) } {P(42,n_ {Flip} ) } \\times( \\mbox{the number of possible sequences of n_H H‚Äôs, n_C C‚ÄôS, n_D D‚Äôs, n_S S‚Äôs} )where the last term was computed in step 1. (This formula is similar to the probability mass function of a binomial random variable . )Step3: Summing these individual probabilities to establish the overall probability for the horse winning, and conversion to oddsSince each combination of cards is independent of another, we can sum the probabilities of all winning combinations for each suit to obtain the overall probability of each horse winning.We can now use these probabilities and the following formula to convert them to odds (of the form numerator:denominator ‚Äúagainst‚Äù; i.e. we expect the horse will lose numerator times for every denominator times it wins):odds=\\frac{1-prob} {prob}Code:Here is some python code that follows this solution (and hence has not be optimised by e.g. vectorising loops) . The functions: choose computes our equation {n\\choose r}:=\\frac{n! } {n! (n-r) ! } (also known as the combination ) perm computes the permutation equation P(r,n):=\\frac{r! } { (r-n) ! } prob_win computes the probability of horse X in the set {H,C,D,S} winning.(Full code and instructions available at GitHub . )def comb(n, r):\"\"\" Return the combination of n and r (i.e. the number of ways for positioning r cards within a total ofn cards (where order doesn't matter). \"\"\"f = math.factorialreturn f(n) / (f(r) * f(n-r))def perm(n, r):\"\"\" Return the permutation of n and r (i.e. the number of ways for positioning r cards within a total ofn cards (where order does matter)). \"\"\"f = math.factorialreturn f(n) / f(n-r)def prob_win(N, X, rX, rY1, rY2, rY3):\"\"\" Let X be in {H,C,D,S} be the horse to win and Y1,Y2,Y3 be in {H,C,D,S}\\{X} be the losing horses.Return the probability that horse X wins the race.N:='number remaining cards (i.e. 42 in usual play)', rX:='number remaining cards for winning suit',rY1:='number remaining cards of first suit not to win', rY2:='number remaining cards of second suitnot to win', rY3:='number remaining cards of third suit not to win'\"\"\"nX = 6 # number of X flipped during race (for X to win)sum_probX = 0 # initial the sum representing the overall probability for X winningnY1 = 0 # number of Y1 flipped during the race (must be less than 6)while nY1 &lt; 6:nY2 = 0while nY2 &lt; 6:nY3 = 0while nY3 &lt; 6:nFlip = nX+nY1+nY2+nY3 # total number of cards flippednPos = nFlip-1 # total number of available positions for flipped cards (-1, noting that that last card must be X for X to win)# number of sequences for a combination of 5 X, nY1 Y1, nY2 Y2 and nY3 Y3nWays = comb(nPos,5) * comb(nPos-5,nY1) * comb(nPos-5-nY1,nY2) * comb(nPos-5-nY1-nY2,nY3)# print \"Number of ways for choosing nX={0}, nY1={1}, nY2={2}, nY3={3} is: {4}\".format(nX,nY1,nY2,nY3,nWays)# probability of observing nX X, nY1 Y1, nY2 Y2 and nY3 Y3 (any order)prob_cards = Decimal( perm(rX,nX) * perm(rY1,nY1) * perm(rY2,nY2) * perm(rY3,nY3) ) / Decimal( perm(N,nFlip) )# print \"prob_cards is: {0}\".format(prob_cards)# probability of X win given a combination of nX X, nY1 Y1, nY2 Y2 and nY3 Y3 (compare with binomial random variables)probX = nWays * prob_cardssum_probX += probXnY3 += 1nY2 += 1nY1 += 1return sum_probXdef prob2odds(p):\"\"\" Return the odds for a given probability p \"\"\"# Note: in the case of the usual game, we do not have to handle impossible events (e.g if a horse cannot win), and so this equation will never result in# divion by zero.return (1-p) / pPost converted from Medium by ZMediumToMarkdown." }, { "title": "NIPS 2017 Summary and Highlights", "url": "/posts/4113030c3794/", "categories": "", "tags": "nips, machine-learning, deep-learning, artificial-intelligence, data-science", "date": "2017-12-14 14:51:03 +0000", "snippet": "NIPS 2017 Summary and HighlightsI was very fortunate to go to the conference on Neural Information Processing Systems ( NIPS ) this year, one of the biggest gathering of AI researchers (indeed, thi...", "content": "NIPS 2017 Summary and HighlightsI was very fortunate to go to the conference on Neural Information Processing Systems ( NIPS ) this year, one of the biggest gathering of AI researchers (indeed, this year‚Äôs tickets were sold out in only a few weeks) . Conferences like NIPS provide great opportunities to learn about most recent research, identify the most important papers (particularly given the sheer number that are released on websites like https://arxiv.org/ every day), as well as tremendous opportunities to discuss techniques and problems with some of the leading experts.Registration queue on Sunday afternoon (the day before the official conference start) !Poster session ‚Äî roughly 230 posters on each Monday, Tuesday and Wednesday evenings.My head is absolutely packed with new ideas, and have come away with lots of methods I want to try and papers to read. Given the sheer amount of research published at this year NIPS, it was impossible to get around every section of the conference. I hope to summarise the conference below, and some of my personal highlights.Some of the key topics from the conference include: deep learning, GANs, unsupervised learning, model interpretability, meta-learning, deep reinforcement learning, probabilistic and Bayesian learning.TutorialsAlthough I hadn‚Äôt registered to attend any of them, it was great that some of them were live-streamed over the NIPS facebook page . The Deep Learning: Practice and Trends tutorial provided a useful overview of deep learning concepts and techniques. It also informed on recent trends in deep learning research and applications: Autoregressive Models: a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. Overview of the types used and the applications (e.g. DeepMind‚Äôs WaveNet for synthesising speech) . Domain Alignment/Unsupervised Alignment: learning to translate from one domain to another, without explicit input/output pairs. One of my favourite papers, Unsupervised Image-to-Image Translation Networks ( code ) uses this idea with generative adversarial networks (GANs) . It works very well at e.g. transferring an image from a winter scene to a summer scene:Translating the winter scene on the left to a summer scene on the right. Example taken from https://www.youtube.com/watch?v=dqxqbvyOnMY&amp;feature=youtu.beDetails behind the key idea of Unsupervised Image-to-Image Translation Networks . Figure taken from the paper. Meta-Learning: where we are ‚Äúlearning-to-learn‚Äù. Graph Networks: investigating how to apply deep learning techniques to graph structures. The tutorial highlighted some interesting ideas, in particular that as data structures like graphs are very general. Thus, being able to take advantage of the benefits of deep learning models on such structures would open up many avenues of research. Program Induction: includes models that automatically learn how to write computer programs using example inputs/outputs.An example of program induction. Image taken from the tutorials slides.Best Paper Awards Safe and Nested Subgame Solving for Imperfect-Information Games (Paper, Video) A Linear-Time Kernel Goodness-of-Fit Test (Code, Paper, Poster) Variance-Bases Regularization with Convex Objectives (Paper)KeynotesI particularly enjoyed John Platt‚Äôs talk discussing how can we power the next 100 years. He argued that fusion will likely be the most important direction for allowing the world to use as much energy as a US resident does today, because energy is essential for a high standard of living. Given the wide variety of fusion research being undertaken, I found it particularly interesting to learn how they went about deciding which technique to focus their research and develop efforts. In particular, their investigation found that the approach the company TAE Technologies use could be most successfully integrated with machine learning techniques.Some of the key reasons why TAE‚Äôs approach to nuclear fusion was chosen ‚Äî should integrate well with machine learning methods. Slide from Platt‚Äôs talk.Brendan Frey‚Äôs talk was on how machine learning is essential to help understand complicated and noisy biology. He also discussed how his company Deep Genomics is reducing the time it takes to develop drugs ‚Äî I was fascinated about the idea of a cloud laboratory, which like cloud technologies for computing allows researchers use remote laboratories to conduct experiments for drug design/test by simply uploading a Python script.Ali Rahmi‚Äôs talk (test-of-time award) discussed how machine learning has become the new alchemy. Instead he wants the ‚ÄúNIPS Rigour Police‚Äù to come back and help take machine learning from alchemy to electricity using ‚Äúsimple experiments and simple theorems [that] are the building blocks that help us understand complicated systems‚Äù. This was certainly one of the most talked about presentation at the entire conference.Kate Crawford presented an important talk on the trouble with bias in machine learning systems. She highlighted how such biases can arise from many factors and thoughts about how to deal with it.Example of bias in Google‚Äôs Translate system ‚Äî ‚ÄúHe is a nurse‚Äù -&gt; [Turkish] -&gt; ‚ÄúShe is a nurse‚Äù (similar scenario seen when using the word ‚Äúdoctor‚Äù) . Slide from Crawford‚Äôs talk.Lise Getoor presented on The Unreasonable Effectiveness of Structure . This introduced me to the concept of statistical relational learning (SRL) which combines principles from probability theory and statistics (to address uncertainty) with tools from knowledge representation and logic (to represent structure) . She also discussed a GitHub tool, Probabilistic Soft Logic (PSL) , for applying such methods.Pieter Abeel presented a very interesting talk on Deep Learning for Robotics . After highlighting some of the successes deep reinforcement learning in particular has had, the bulk of his talk discussed the key problems that exist is robotics: Faster Reinforcement Learning: in particular, humans can often learn tasks much more quickly than the training required for existing DRL algorithms. Long Horizon Reasoning: the ability to reason and plan over a long time span. Taskability: the ability to tell robots what we want them to do. One such approach is to use imitation learning. Lifelong Learning: the ability to learn continuously (i.e. during deployment) . Leverage Simulation: simulation is less expensive, dangerous, and is much faster/scalable to train that physical robots. Maximise Signal Extracted from Read World Experience: we what to ensure we are extracting as much real world signal from our (expensive) data collected from physical robots.Yael Niv delivered a talk with a more neuroscience focus on Learning State Representations . She asked the question how do we learn from relatively little experience. I found some of the experiments conducted very fascinating.Finally, Yee Whye Teh presented On Bayesian Deep Learning and Deep Bayesian Learning . Bayesian techniques was a big theme at NIPS, particularly methods for helping us to better reason about uncertainty and learn more efficiently from smaller datasets.Great slide from Yee Whye Teh keynote On Bayesian Deep Learning and Deep Bayesian Learning , comparing techniques from the two approaches.Bayesian Deep Learning uses a Gaussian prior on the weights of a neural network.Some of my favourite bitsSafe and Nested Subgame Solving for Imperfect-Information Games : This paper discusses the system called Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold‚Äôem poker. Imperfect-information games are games where information is hidden from players (unlike games such as Go or Chess) . This is important as there are many real-world decision-making situations that only have imperfect information. The accompanying video provides a helpful overview.Dynamic Routing Between Capsules : One of the most anticipated paper published at this year‚Äôs NIPS was Geoff Hinton‚Äôs idea of ‚Äúcapsules‚Äù (lead authors for this particular paper were Sara Sabour and Nicholas Frosst) . Capsule Networks try to address some of the short comings of Convolutional Neural Networks. Indeed a ‚Äúdiscrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits‚Äù. The paper also states ‚Äúresearch on capsules is now at a similar stage to research on recurrent neural networks for speech recognition at the beginning of this century‚Äù. I found the accompanying video gives a great overview, and this blog post gives an excellent explanation behind the intuition and motivation behind them.Capsule Networks try to address some of the short comings of Convolutional Neural Networks. Indeed a ‚Äúdiscrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits‚Äù. Image from video.Model Interpretability: understanding how your model makes predictions is as important as their accuracy. There were various papers attempting to approach this. One talk that I found particularly interesting was Streaming Weak Submodularity: Interpreting Neural Networks on the Fly . This paper introduces the STREAK algorithm , which not only identifies the parts of an image most important for explaining a prediction, but also does this much faster than previous algorithms.STREAK algorithm - not only identifies the parts of an image most important for explaining a prediction, but also does this fast. Slide from the talk.Another approach that aims to improve model interpretability was SHAP (SHapley Additive exPlanation) , which unifies aspects of several previous approaches.Deep reinforcement learning: David Silver provided yet another fascinating talk on their latest game-playing system. Announced only earlier in the week of the conference, Silver discussed the evolution from AlphaGo to AlphaZero, which learns entirely from self-play (plus the rules of a game) . They made AlphaZero even more general, allowing it to master not only Go, but also Chess and Shogi (Japanese Chess) .AlphaZero surpassed all previous masters (fast! )Unlike StockFish, the leading chess game engine which uses many hand-crafted game plays, AlphaZero learns in a much more general way primarily driven through self-play ‚Äî interestingly AlphaZero did learn some of the well-know moves in the slide above during training.Another aspect I found fascinating with AlphaZero was how it also learned completely new game play that have not be discovered previously. This included weird behaviour where AlphaZero may e.g. sacrifice a Queen, only for it to be seen (many moves later) why it took such an action.In his talk Ilya Sutskever hypothesised that self-play will be a key aspect to AGI.Other code implementations I‚Äôm interested in trying: A Bayesian Data Augmentation Approach to Learning Deep Models (Keras) Mean teachers are better role models (PyTorch and TensorFlow) ‚Äî a state-of-the-art semi-supervised method for image recognition. ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games Sharkzor: Interactive Deep Learning for Image Triage, Sort and Summary ‚Äî although not currently available (the author mentioned they intend to open source this soon), this demonstrated a fantastic web application for machine-learning assisted image classification . I‚Äôm very much looking forward to playing with this tool. I also believe it will provide a great platform for non-technical people to perform image classification.Miscellaneous A very comprehensive set of notes for NIPS 2017 . Here is an interesting visualisation of the 737 Twitter accounts most followed by members of the NIPS2017 community . http://www.matrixcalculus.org/ ‚Äî enables you to compute matrix derivatives using symbolic differentiation.Other cool results I came across but not necessarily presented at NIPS Don‚Äôt decay the learning rate, increase the batch size ‚Äî results show that you should see the same learning curve. Distilling a Neural Network Into a Soft Decision Tree ‚Äî using a trained deep neural network to create a type of soft decision tree, helping to explain why it makes a particular classification decision on a particular test case. Michael Nielsen‚Äôs video providing intuition behind the universal approximation theorem for neural networks.Any thoughts/corrections welcome!Post converted from Medium by ZMediumToMarkdown." } ]
